---
title: "Mixed models introduction"
author: "Timoth√©e Bonnet"
date: "12 June 2018"
output:
  html_document: 
    toc: true
    toc_float: true
  pdf_document: default
---

This document provide an introduction to linear mixed models through examples and exercises.

## An example of why mixed models are useful

We first simulate data that violate an assumption of linear models: independance of residuals.

### Data simulation

The following code may appear superficially complicated, but don't worry about it. This tutorial is about mixed models, not simulations, you can just copy-paste it to generate the data we will need later on.

```{r}
set.seed(1234)
x <- 4+sort(rnorm(100)) + rnorm(100,0,0.15)
block <- rep(x = 1:5, each=20)
blockvalues <- c(-2,-1,0,1,2)
y <- 8 - x + blockvalues[block] + rnorm(100,0,0.5)
dat <- data.frame(response = y, predictor = x, block=block)
```

Is there an effect of the predictor (let's pretend Thorn density in a bush) on the response (let's pretend Herbivory load on the bush)?
Graphically, it looks like there is a positive relationship (more thorns means more herbivory!?):
```{r}
plot(dat$response, x=dat$predictor, ylab = "Herbivory load", xlab= "Thorn density")
abline(lm(response~ predictor, data=dat), lwd=5, col="gray")#this is a shortcut to draw a regression line
```

The summary of a linear model (simple linear regression) also indicates that strong evidence for a positive relationship between the response and the predictor.
```{r}
summary(lm(response~ predictor, data=dat))
```

As always, we should check the assumptions of our model to be confident about the parameter estimates and their uncertainty (in particular, their p-values). The main assumptions are:
* A Gaussian distribution of the residuals
* A constant variance in residuals
* Independence of residuals

You can have a look at some diagnostic graphes using plot(lm()):
```{r, eval=FALSE}
plot(lm(response~ predictor, data=dat))
```

You should notice that the two first assumptions are more or less OK, but that something is wrong with the third one: there are trends in the residuals, so that knowing the value of a residual makes you able to more or less predict where should be the next one.
**That is a serious warning.**
But you will not always be that lucky. Often, these graphes would not reveal such strong patterns even when the problem is accute. 
Instead, you should think about how you collected the data and what may cause the residuals to be non-independent.

Let's pretend that we collected the data from five different locations (blocks).
What if we visualizing differences between locations:

```{r}
plot(x,y, col=block, ylab = "Herbivory load", xlab= "Thorn density")
abline(lm(response~ predictor, data=dat), lwd=5, col="gray")
```

It looks like there is a negative effect of thorn density *within* blocks, but that it is hidden by *among* block differences!

The non-independence of residuals is caused by differences in herbivory among blocks, which hides the real effect of thorns on herbivory.
(NB: don't get confused, in this case the blocks also differ in the predictor (thorn density), but that is not important; the problem is differences  in the **response**.)

How to correct for this problem statistically?

One option would be to fit five regressions, one per-site. However, that would substantially reduce the statistical power, and you would likely not detect the relationship in all the blocks even if it is there in all of them.



```{r}
summary(lm(response~ predictor + as.factor(block), data=dat))
```


```{r}
summary(lmer(response ~ predictor + (1|block), data=dat))
```


### A more extreme example, where a random effect is really better than fixed effects

```{r}
set.seed(1234)
sampsize <- 300
x <- rnorm(sampsize,mean = 4, sd=0.25)
nbblocks <- 50
block <- sample(x = 1:nbblocks, size = sampsize, replace = TRUE)
blockvalues <- rnorm(n = nbblocks, mean = 0, sd = 3)
y <- 8 - x + blockvalues[block] + rnorm(sampsize,0,1)
dat <- data.frame(response = y, predictor = x, block=block)
```

The true effect of the predictor on the response is -1.

```{r}
summary(lm(response ~ predictor, data=dat))
```
A simple linear regression greatly over-estimate the effect, and the standard error is very large.


Compare the linear model with block as a factor, and the mixed model with block as a random effect:
```{r, eval=FALSE}
summary(lm(response ~ predictor + as.factor(block), data=dat))

summary(lmer(response ~ predictor + (1|block), data=dat))
```

They both provide quite a good estimate for the effect of predictor with rather small standard errors (actually the estimates and standard errors should be almost identical).
However, the linear model is a mess, there are many parameters we don't care about! These blocks are different for many reasons we don't have any idea about. Does it matter if block 4 is significantly different from block 1 but block 5 isn't? Probably not.
The mixed model returns only one variance component for block, instead of 49 estimates for the different levels of the factor.
The variance component is a simple measure of how much blocks differ.




## Testing random effect significance



## Other mixed-model packages are very cool

### glmmTMB
```{r}
library(glmmTMB)
```

### MCMCglmm
```{r}
library(MCMCglmm)
```

### brms

```{r}
library(brms)
library(shinystan)
```



