<style>
.section {
    color: black;
    background: #E9E8E8;
    position: fixed;
    text-align:center;
    width:100%;
}
</style>
<style>
.small-code pre code {
  font-size: 1.2em;
}
</style>
Introduction to linear mixed models
========================================================
author: timotheenivalis.github.io
date: June 15 2018
autosize: true
font-family: 'Helvetica'

Example: hidden relationships
========================================================
class: small-code

```{r}
thorns <- read.table(file = "thorndata.txt", header=TRUE)
```

```{r}
plot(thorns$response, x=thorns$predictor, ylab = "Herbivory load", xlab= "Thorn density")
abline(lm(response~ predictor, data=thorns), lwd=5, col="gray")#this is a shortcut to draw a regression line
```


Example: hidden relationships
========================================================
class: small-code

```{r}
lmthorns <- lm(response~ predictor, data=thorns)
summary(lmthorns)
```

Example: hidden relationships
========================================================

```{r}
plot(lmthorns, which=1)
```

Example: hidden relationships
========================================================
class: small-code
*Simpson's paradox*

```{r}
plot(thorns$predictor, thorns$response, col=thorns$block, ylab = "Herbivory load", xlab= "Thorn density")
abline(lm(response~ predictor, data=thorns), lwd=5, col="gray")
```

Example: hidden relationships
========================================================
class: small-code

Fixed-effect correction
```{r}
summary(lm(response~ predictor + as.factor(block), data=thorns))
```

Example: hidden relationships
========================================================
class: small-code

```{r, message=FALSE, warning=FALSE}
library(lme4)
thornLMM <- lmer(response ~ predictor + (1|block), data = thorns)
summary(thornLMM)
```


Example: hidden relationships
========================================================
class: small-code
```{r, echo=FALSE}
plot(thorns$predictor, thorns$response, col=thorns$block, ylab = "Herbivory load", xlab= "Thorn density")
slp <- fixef(thornLMM)[2]
inter <- fixef(thornLMM)[1]
re <- ranef(thornLMM)$block[,1]

abline(a = inter+re[1], b=slp, lwd=5)
abline(a = inter+re[2], b=slp, lwd=5, col="red")
abline(a = inter+re[3], b=slp, lwd=5, col="green")
abline(a = inter+re[4], b=slp, lwd=5, col="blue")
abline(a = inter+re[5], b=slp, lwd=5, col="cyan")
```

Exercise: more hidden relationships
========================================================
type: prompt

Load the data _thornsmanylocations.txt_

Compare **lm()** and **lmer()** corrections for block.



What are random effects?
========================================================
type: section

Residuals and random effects
========================================================

```{r, echo=FALSE}
resv <- as.data.frame(diag(5), row.names = c("obs1", "obs2", "obs3", "obs4", "obs5"))
names(resv) <- c("obs1", "obs2", "obs3", "obs4", "obs5")
```

Assumed variance-covariance matrix of the process that generates the residuals
```{r, echo=FALSE}
knitr::kable(resv)
```

*residuals are perfectly correlated with themselves, and independent of each-other*


Residuals and random effects
========================================================

```{r, echo=FALSE}
resv <- as.data.frame(diag(5), row.names = c("obs1", "obs2", "obs3", "obs4", "obs5"))
names(resv) <- c("obs1", "obs2", "obs3", "obs4", "obs5")
resv <- rbind(c("ind1", "ind1", "ind2", "ind2", "ind3"),resv)
resv <- cbind(c("","ind1", "ind1", "ind2", "ind2", "ind3"),resv)
dimnames(resv) <- list(c("","obs1", "obs2", "obs3", "obs4", "obs5"),c("","obs1", "obs2", "obs3", "obs4", "obs5"))
resv[2,3] <- 1
resv[3,2] <- 1
resv[5,4] <- 1
resv[4,5] <- 1
```

If multiple measurements:
```{r, echo=FALSE}
knitr::kable(resv)
```

*among individuals, residuals are correlated with each-other*

Residuals and random effects
========================================================
left: 35%

```{r, echo=FALSE}
idv <- as.data.frame(diag(3), row.names = c("ind1", "ind2", "ind3"))
names(idv) <- c("ind1", "ind2", "ind3")
```

**Individual var-cov**
```{r, echo=FALSE}
knitr::kable(idv)
```
***
**Residual var-cov**
```{r, echo=FALSE}
resv <- as.data.frame(diag(5), row.names = c("obs1", "obs2", "obs3", "obs4", "obs5"))
names(resv) <- c("obs1", "obs2", "obs3", "obs4", "obs5")
```

```{r, echo=FALSE}
knitr::kable(resv)
```


NB: phylogenies, spatial correlation, time-series, genetic similarity...
========================================================

...are mixed models with correlations across random effect levels (individuals, species, locations...)
```{r, echo=FALSE}
resv <- as.data.frame(diag(5), row.names = c("obs1", "obs2", "obs3", "obs4", "obs5"))
names(resv) <- c("obs1", "obs2", "obs3", "obs4", "obs5")
resv <- rbind(c("ind1", "ind1", "ind2", "ind3", "ind4"),resv)
resv <- cbind(c("","ind1", "ind1", "ind2", "ind3", "ind4"),resv)
dimnames(resv) <- list(c("","obs1", "obs2", "obs3", "obs4", "obs5"),c("","obs1", "obs2", "obs3", "obs4", "obs5"))
resv[2,3] <- 1
resv[3,2] <- 1
resv[2,4] <- 0.25
resv[4,2] <- 0.25
resv[2,6] <- 0.01
resv[6,2] <- 0.01
resv[6,5] <- 0.125
resv[5,6] <- 0.125
```

```{r, echo=FALSE}
knitr::kable(resv)
```

Correlations represent **(co-)variation that is unexplained**, but is **related to a biological process**


Residuals and random effects
========================================================

$$
y_{ij} = \mu + \beta x_{ij} + u_i + \epsilon_{ij}
$$
with, residuals $\epsilon_{ij}\sim Normal(0,V_R)$ and individual random effect $u_{i}\sim Normal(0,V_I)$.


Random effects are cool because:
* More efficient than estimating many independent fixed effects
* Avoid distration from many coefficients and p-values
* Test effect of grouping variable as one parameter
* Variance components biologically interesting (e.g. repeatability $V_I/(V_I+V_R)$)


Testing random effects significance
========================================================
type: section

Likelihood Ratio Test (LRT)
========================================================
class: small-code

Comparison of two nested models. Ratio of likelihood $\sim \chi^2$

```{r}
thornLMM <- lmer(response ~ predictor + (1|block), data = thorns)
thornLM <- lm(response ~ predictor, data = thorns)
anova(thornLMM, thornLM) # the mixed model MUST GO FIRST
```



Exercise: LRT p-value distribution
========================================================
type: prompt
class: small-code

One simulation with a random effect that has no effect
```{r}
set.seed(1234)
RandomVariance <- 0
sampsize <- 200
x <- rnorm(sampsize,mean = 4, sd=0.25)
nbblocks <- 50
block <- sample(x = 1:nbblocks, size = sampsize, replace = TRUE)
blockvalues <- rnorm(n = nbblocks, mean = 0, sd = sqrt(RandomVariance))
y <- 8 - x + blockvalues[block] + rnorm(sampsize,0,1)
dat <- data.frame(response = y, predictor = x, block=block)
```

```{r, eval=FALSE}
lm0 <- lm(response ~ 1 + predictor, data=dat)
lmm0 <- lmer(response ~ 1 + predictor + (1|block), data=dat )
(LRT0 <- anova(lmm0, lm0)) #mixed model must come first!
LRT0$`Pr(>Chisq)`[2] # the p-value
```

Exercise: LRT p-value distribution
========================================================
type: prompt

**Replicate the simulations to obtain the distribution of p-values under the null-model of no variance**

```{r, echo=FALSE, eval=FALSE}
set.seed(1234)
RandomVariance <- 0
sampsize <- 200
nbblocks <- 50
nbsimuls <- 1000
pvalues <- vector(length = nbsimuls)
for (i in 1:nbsimuls)
{
  x <- rnorm(sampsize,mean = 4, sd=0.25)
  block <- sample(x = 1:nbblocks, size = sampsize, replace = TRUE)
  blockvalues <- rnorm(n = nbblocks, mean = 0, sd = sqrt(RandomVariance))
  y <- 8 - x + blockvalues[block] + rnorm(sampsize,0,1)
  dat <- data.frame(response = y, predictor = x, block=block)
  lm0 <- lm(response ~ 1 + predictor, data=dat)
  lmm0 <- lmer(response ~ 1 + predictor + (1|block), data=dat )
  (LRT0 <- anova(lmm0, lm0)) #mixed model must come first!
  pvalues[i] <- LRT0$`Pr(>Chisq)`[2] # the p-value
}

hist(pvalues)
mean(pvalues<0.05) 
```


Correct p-values in LRT
========================================================
incremental: true
A variance cannot be negative
```{r, eval=FALSE}
confint(lmm0) #Confidence interval
```

LRT are two sided tests / count one parameter per random effect
A random effect is half a parameter / to be tested with one-side tests

**Divide the p-values by two**

Same problem with AIC/BIC: count only half a parameter per random effect
**Remove one IC point per random effect**

*NB: it is more complicated with random interactions; but the rule is to count half a parameter by variance parameter*

Test / remove non-significant random effects?
========================================================
incremental: true

**Test ?**

* Yes if effect of interest
* Maybe if only a nuisance parameter

**Remove ?**

* Probably should keep if part of exp. design
* Doesn't matter much if non-significant
* Maybe remove if too many variables in exploratory analyses

Beyond the random intercept
========================================================
type: section

Random interactions, random slopes...
========================================================

```{r, echo=FALSE}
plot(thorns$predictor, thorns$response, col=thorns$block, ylab = "Herbivory load", xlab= "Thorn density", main="Random intercept only")
slp <- fixef(thornLMM)[2]
inter <- fixef(thornLMM)[1]
re <- ranef(thornLMM)$block[,1]

abline(a = inter+re[1], b=slp, lwd=5)
abline(a = inter+re[2], b=slp, lwd=5, col="red")
abline(a = inter+re[3], b=slp, lwd=5, col="green")
abline(a = inter+re[4], b=slp, lwd=5, col="blue")
abline(a = inter+re[5], b=slp, lwd=5, col="cyan")
```

Random interactions, random slopes...
========================================================

```{r, echo=FALSE}
plot(thorns$predictor, thorns$response, col=thorns$block, ylab = "Herbivory load", xlab= "Thorn density", main="Random intercept and slope")
thornLMM2 <- lmer(response ~ 1 + predictor + (1+predictor|block), data=thorns)
slp <- fixef(thornLMM2)[2]
inter <- fixef(thornLMM2)[1]
reint <- ranef(thornLMM2)$block[,1]
resl <- ranef(thornLMM2)$block[,2]

abline(a = inter+reint[1], b=slp+resl[1], lwd=5)
abline(a = inter+reint[2], b=slp+resl[2], lwd=5, col="red")
abline(a = inter+reint[3], b=slp+resl[3], lwd=5, col="green")
abline(a = inter+reint[4], b=slp+resl[4], lwd=5, col="blue")
abline(a = inter+reint[5], b=slp+resl[5], lwd=5, col="cyan")
```

Random interactions, random slopes...
========================================================
incremental: true

"Random interaction" predictor:block = "random slope" = "random regression"
```{r, eval=FALSE}
lmer(response ~ 1 + predictor + (1+predictor|block), data=thorns)
```

Blocks allowed to differ in intercept and slopes

Fits 2 variances and 1 covariance

Syntax to more random effects:
http://bbolker.github.io/mixedmodels-misc/glmmFAQ.html#model-specification

Package demonstration
========================================================
type: section

Data
========================================================
type: prompt

Load data to experiment with various packages
```{r, echo=FALSE, eval=FALSE}
nobs <- 1200
x <- rnorm(n = nobs, mean = 0, sd = 1)
nind <- 120
indobs <- sample(x = 1:nind, size = nobs, replace = TRUE)
indval <- rnorm(n = nind, mean = 0, sd = sqrt(0.6))
indslp <- rnorm(n=nind, mean=0, sd=sqrt(0.01))
y <- 25+ (0.2+indslp[indobs])*x + indval[indobs] +rnorm(nobs, 0, 2)

dat <- data.frame(response=y, predictor=x, individual=indobs)
write.table(dat, file = "datforpackagecomp.txt", quote = FALSE, row.names = FALSE)
```

```{r}
dat <- read.table(file = "datforpackagecomp.txt", header=TRUE)
```

The simulated intercept variance among individual is 0.6.
The simulated slope variance among individual is 0.01.
The simulated effect of the predictor on the response is 0.2

lme4
========================================================
incremental: true
class: small-code

Standard, fast, simple package
```{r}
library(lme4)
```

```{r}
mlme4 <- lmer(response ~ 1 + predictor + (1|individual), data=dat)
summary(mlme4)
```

lme4
========================================================
incremental: true
class: small-code

No p-values (for good reason) if you really want them:

```{r}
library(lmerTest)
summary(lmerTest::lmer(response ~ 1 + predictor + (1|individual), data=dat))
```

lme4
========================================================
incremental: true
class: small-code

Rudimentary diagnostic
```{r, eval=FALSE}
plot(mlme4)
```

S4, complicated components:
```{r, eval=FALSE}
mlme4@beta
mlme4@u
```

Better use functions to extract components:
```{r, eval=FALSE}
fixef(mlme4)
ranef(mlme4)
VarCorr(mlme4)
```

Individual repeatability:
```{r}
as.numeric(VarCorr(mlme4)$individual)/sum(getME(mlme4, "sigma")^2, as.numeric(VarCorr(mlme4)$individual))
```

glmmTMB
========================================================
incremental: true
class: small-code


In development, more options (e.g. Zero-Inflation) sometimes but a bit slower (lmm)/faster(glmm) than lme4, fewer diagnostic, less easy to extract coeff

```{r, eval=FALSE}
install.packages("glmmTMB")
```

```{r}
library(glmmTMB)
```

```{r}
mglmmtmb <- glmmTMB(response ~ 1 + predictor + (1|individual), data=dat)
summary(mglmmtmb)
```

glmmTMB
========================================================
incremental: true
class: small-code


No automated diagnostics:
```{r, eval=FALSE}
plot(mglmmtmb) #DOESN'T work yet
```

Estimate extraction from summary (other ways?)

Individual repeatability:
```{r}
smglmmtmb <- summary(mglmmtmb)

as.numeric(smglmmtmb$varcor$cond$individual)/(smglmmtmb$sigma^2 + as.numeric(smglmmtmb$varcor$cond$individual))
```


MCMCglmm
========================================================
class: small-code

Bayesian MCMC, slow compare to ML, more flexible, estimate better complicated problems, post-treatment very easy and statistically correct

```{r, eval=FALSE}
install.packages("MCMCglmm")
```

```{r}
library(MCMCglmm)
```

```{r, eval=TRUE}
mmcmcglmm <- MCMCglmm(fixed = response ~ 1 + predictor,
                      random =  ~individual, data=dat)
summary(mmcmcglmm)
```

MCMCglmm
========================================================
incremental: true
class: small-code

Diagnostics:

```{r, eval=FALSE}
plot(mmcmcglmm)
autocorr(mmcmcglmm$VCV)
summary(mmcmcglmm$VCV)
```

Repeatability
```{r, results='hide'}
mcmcmrep <- mmcmcglmm$VCV[,"individual"] / (mmcmcglmm$VCV[,"individual"] + mmcmcglmm$VCV[,"units"])
plot(mcmcmrep)
```

```{r}
posterior.mode(mcmcmrep)
HPDinterval(mcmcmrep)
```

brms
========================================================
class: small-code

Bayesian Hamiltonian Monte Carlo based on STAN, very slow, but super efficient estimation, "infinitely" flexible (by modifying the STAN code)

```{r, eval=FALSE}
install.packages("brms")
install.packages("shinystan")
```
```{r}
library(brms)
library(shinystan)
```

```{r, eval=FALSE}
mbrms <- brm(formula = response ~ 1 + predictor + (1|individual), data=dat)
summary(mbrms)
```
Diagnostics:
```{r, eval=FALSE}
plot(mbrms)
launch_shinystan(mbrms)
```

brms
========================================================
class: small-code

```{r, eval=FALSE}
fixef(mbrms)
ranef(mbrms)
brms::VarCorr(mbrms)
```

Repeatability
```{r, eval=FALSE}
repbrms <- posterior_samples(mbrms, pars = "sd")^2 /(posterior_samples(mbrms, pars = "sd")^2 +posterior_samples(mbrms, pars = "sigma")^2) 
plot(as.mcmc(repbrms))
```

INLA
========================================================
class: small-code

Bayesian Laplace Approximation, very fast, good estimation, not as flexible.

```{r, eval=FALSE}
install.packages("INLA", repos=c(getOption("repos"), INLA="https://inla.r-inla-download.org/R/stable"), dep=TRUE)
```

```{r}
library(INLA)
```

```{r, eval=FALSE}
minla <- inla(formula =response ~ 1 + predictor + f(individual, model = "iid"), data=dat )
summary(minla)
```

Summary (based on personal exp.)
=======================================================
```{r, echo=FALSE}
comp <- data.frame(Package= c("**lme4**", "**glmmTMB**", "**MCMCglmm**", "**brms**", "**INLA**"),
                   Framework = c("ML", "ML", "Bayes", "Bayes", "Bayes"),
                   Speed = c("fast", "fast+", "very slow", "slow", "fast-"),
                   Flexibility = c("-", "+", "++", "+++", "++"),
                   Syntax = c("lme4", "lme4", "+/- lme4", "lme4", "diff"),
                   Doc = c("good", "low", "medium+ (formal)", "medium (blogs)", "low"))
```

```{r, echo=FALSE}
knitr::kable(comp)
```

```{r, echo=FALSE}
comp <- data.frame(Package= c("**lme4**", "**glmmTMB**", "**MCMCglmm**", "**brms**", "**INLA**"),
                   Post_treatment = c("difficult", "difficult", "easy but manual", "easy(?) and automated", "medium and manual"),
                   Whims = c("few", "some-", "few", "some", "some+"),
                   Structure = c("S4", "S3", "S3", "S3", "S3"))
```

```{r, echo=FALSE}
knitr::kable(comp)
```


Exercise: Fit random regressions with the packages you like
====================================================
type: prompt


Exercise: Compare the speed of the different packages
====================================================
type: prompt

And let me know what you find!

Hint:
```{r, eval=FALSE}
system.time()
```

Essential resources
====================================================

Ben Bolker FAQ:
http://bbolker.github.io/mixedmodels-misc/glmmFAQ.html
http://glmm.wikidot.com/start

Subscribe to mailing-list:
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

MCMCglmm:
https://cran.r-project.org/web/packages/MCMCglmm/vignettes/CourseNotes.pdf

brms:
https://paul-buerkner.github.io/blog/brms-blogposts/
http://elevanth.org/blog/2017/11/28/build-a-better-markov-chain/

