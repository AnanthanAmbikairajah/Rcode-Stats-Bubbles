\documentclass[10pt]{beamer}%
\usetheme{Boadilla}
\usecolortheme{seahorse}

\usepackage[utf8]{inputenc}%


\usepackage[normalem]{ulem}%strikeout
 

% graphics
%% Figures %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{graphicx}
\usepackage{xcolor}%for color mixing

\usepackage{amsmath}%
\usepackage{amsfonts}%
\usepackage{amssymb}%
\usepackage{graphicx}

\usepackage{tikz}
\usetikzlibrary{calc}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%% Doc info %%%%%%%%%%%%%%%%%%%
\title[\textbf{Generalized linear models:}]{Generalized Linear Models (GLMs)}
\date{\today}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}

<<Plot Options, echo=FALSE, message=FALSE>>=
#load(file = ".RData")
opts_knit$set(width=60)
opts_chunk$set(comment=NA, fig.width=8, fig.height=6, out.width='0.8\\textwidth',
               out.height='0.6\\textwidth',background='#D7DDEB', size="small")


szgr <- 2
szax <- 1.3
marr <- c(4, 4, 1, 1) + 0.1
setPar<-function(){
par(las=1,mar=marr, cex=szgr, cex.lab=szax , cex.axis=szax, lwd=2 ,pch=1, las=1)
}
setPar()
@


\begin{frame}
\maketitle
\end{frame}
%%%%%%%%%%%

\AtBeginSection[]
{
  \begin{frame}<beamer>
    \frametitle{}
    \tableofcontents[currentsection,hideothersubsections,subsectionstyle=hide]% down vote\tableofcontents[currentsection,currentsubsection,hideothersubsections,sectionstyle=show/hide,subsectionstyle=show/shaded/hide]
  \end{frame}
}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Linear model, reminder}

\begin{frame}[fragile]{A simple linear model}
  \textbf{{\color{purple}{Response}} = {\color{blue}{Intercept}} + {\color{red}{Slope}} $\times$ {\color{orange}{Predictor}} + {\color{gray}{Error}}} \\

  <<lmprinc, echo=FALSE, dev='tikz'>>=
    setPar()
    set.seed(123)
    x <- rnorm(20)
    y <- 1 + x + rnorm(20)
    plot(x, y, xlab="\\color{orange}{Predictor}", ylab="\\color{purple}{Response}")
    lm0 <- lm(y~x)
    abline(lm0, col="red", lwd=5)
    abline(h=coef(lm0)[1], lty=2, col="blue", lwd=5)
    abline(v=0)
    abline(h=0)

    arrows(x0 = x, y0=y, y1=lm0$fitted.values, code=0, col="gray", lwd=3)
  @
\end{frame}
%%%%%%%%%%%

\begin{frame}[fragile]{A simple linear model failure: binary data}

  <<binlmprinc, echo=FALSE, dev='tikz'>>=
    setPar()
    set.seed(123)
    x <- rnorm(30)
    latent <- 1 + 2*x + rnorm(30, sd = 0.5)
    y <- 1/(1+exp(-latent))
    obs <- sapply(y, FUN=function(x){rbinom(1,1,x)})
    plot(x, obs, xlab="\\color{orange}{Predictor}", ylab="\\color{purple}{Response}", xlim = c(-3,3), ylim=c(-0.5,1.5))
    lm0 <- lm(y~x)
    abline(lm0, col="red", lwd=5)
    abline(h=coef(lm0)[1], lty=2, col="blue", lwd=5)
    abline(v=0)
    abline(h=0)

    arrows(x0 = x, y0=obs, y1=lm0$fitted.values, code=0, col="gray", lwd=3)
  @
  
\end{frame}
%%%%%%%%%%%

\begin{frame}{Linear model basic assumptions}
 \begin{block}{}
     \begin{itemize}
      \item Linear combination of parameters (including transformation, polynoms, interactions\dots)\\ \textit{Risk: biologically meaningless}
      \item Predictor not perfectly correlated \\ \textit{Risk: Model won't run, unstable convergence, or huge SE}
       \item {\color{red!20!black}{Little error in predictors}}\\ \textit{Risk: bias estimates (underestimate with Gaussian error)}
       \item {\color{red!50!black}{Gaussian error distribution}}\\ \textit{Risk: Poor predictions}
       \item {\color{red!70!black}{Homoscedasticity (constant error variance)}}\\ \textit{Risk: Over-optimistic uncertainty, unreliable predictions}
       \item {\color{red!99!black}{Independence of error}}\\ \textit{Risk: Bias and over-optimistic uncertainty}
     \end{itemize}
 \end{block}
\end{frame}
%%%%%%%%%%%

\begin{frame}[fragile]{A simple linear model failure: binary data}

  <<binlmprinc2, echo=FALSE, dev='tikz', out.width="0.7\\textwidth", out.height="0.53\\textwidth">>=
    setPar()
    plot(x, obs, xlab="\\color{orange}{Predictor}", ylab="\\color{purple}{Response}", xlim = c(-3,3), ylim=c(-0.5,1.5))
    abline(lm0, col="red", lwd=5)
    abline(h=coef(lm0)[1], lty=2, col="blue", lwd=5)
    abline(v=0)
    abline(h=0)
    arrows(x0 = x, y0=obs, y1=lm0$fitted.values, code=0, col="gray", lwd=3)
  @
  
  \begin{alertblock}{Assumptions violated:}
    Non-Gaussian errors, non-constant error variance, correlated errors
  \end{alertblock}
\end{frame}
%%%%%%%%%%%

\begin{frame}[fragile]{A simple linear model failure: binary data}

  <<binlmprinc3, echo=FALSE, dev='tikz', out.width="0.7\\textwidth", out.height="0.53\\textwidth">>=
    setPar()
    plot(x, obs, xlab="\\color{orange}{Predictor}", ylab="\\color{purple}{Response}", xlim = c(-3,3), ylim=c(-0.5,1.5))
    abline(lm0, col="red", lwd=5)
    abline(h=coef(lm0)[1], lty=2, col="blue", lwd=5)
    abline(v=0)
    abline(h=0)
    arrows(x0 = x, y0=obs, y1=lm0$fitted.values, code=0, col="gray", lwd=3)
    ndat <- data.frame(x=seq(-2.9,2.9, length.out = 100))
    ndat <- cbind(ndat, predict(lm0, newdata =ndat, interval = "prediction"))
    polygon(x=c(ndat$x, rev(ndat$x)), y=c(ndat$lwr, rev(ndat$upr)), border = NA, col=rgb(0.5,0,0,0.4))
  @
  \begin{alertblock}{Practical consequences:}
    Non-sensical predictions, wrong confidence-interval and p-value, extrapolation ALWAYS fails
  \end{alertblock}
\end{frame}
%%%%%%%%%%%

\begin{frame}[fragile]{What we want our model to do}
  <<binglmprinc, echo=FALSE, dev='tikz', out.width="0.7\\textwidth", out.height="0.53\\textwidth">>=
    setPar()
    plot(x, obs, xlab="\\color{orange}{Predictor}", ylab="\\color{purple}{Response}", xlim = c(-3,3), ylim=c(-0.5,1.5))
    glm0 <- glm(obs~x, family="binomial")
    ndat <- data.frame(x=seq(-2.9,2.9, length.out = 100))
    ndat <- cbind(ndat, predict.glm(glm0, newdata =ndat, se.fit = TRUE))
    ndat$lci <- ndat$fit - 1.96*ndat$se.fit
    ndat$hci <- ndat$fit + 1.96*ndat$se.fit
      
    lines(ndat$x, 1/(1+exp(-ndat$fit)), col="red", lwd=5)
    abline(h=1/(1+exp(-coef(glm0)[1])), lty=2, col="blue", lwd=5)
    abline(v=0)
    abline(h=0)
   arrows(x0 = x, y0=obs, y1=1/(1+exp(-predict(glm0))), code=0, col="gray", lwd=3)
    polygon(x=c(ndat$x, rev(ndat$x)), y=1/(1+exp(-c(ndat$lci, rev(ndat$hci)))), border = NA, col=rgb(0.5,0,0,0.4))

  @
    \begin{alertblock}{Good features:}
    Never out of [0,1], variable uncertainty, non-linear trend, close fit
  \end{alertblock}
\end{frame}
%%%%%%%%%%%

\begin{frame}{That is what a Generalized Linear Model does}

\begin{block}{Vocabulary warning}
  \begin{itemize}
    \item General Linear Model (=linear model with several responses, multivariate)
    \item \textbf{Generalized Linear Model (=non-normal errors, and uncertainty dependent on the mean)} 
  \end{itemize}
\end{block}

\pause

\begin{block}{What a GLM is:}
  \begin{enumerate}
    \item A linear function ($y = \mu + \beta x$ \dots)
    \item A probability distribution (Bernouilli, Binomial, Poisson\dots)
    \item A "link function" to convert between the scale of the linear function ($-\infty$ to $+\infty$) and the scale of the data and the probability distribution (often positive integer: 0, 1, 2, 3\dots)
  \end{enumerate}
  A GLM fits a continuous expected response; we observe discrete realizations
\end{block}

\end{frame}
%%%%%%%%%%%

\begin{frame}[fragile]{Logistic regression}
  \begin{itemize}
    \item Binary or proportion data (survival, presence/absence\dots)
    \item Binomial probability distribution ( = Bernouilly if binary data)
    \item Link function often logit: $y=\log(\frac{probability}{1-probability})$
    \item Back-transformation inverse-logit: $probability = \frac{1}{1 + exp(-y)}$
    \item Linear function $y = intercept + slope_1 predictor_1 + slope_2 predictor_2 +$ \dots
  \end{itemize}

\pause
  Binomial (and Bernouilli distribution in R):
  <<eval = FALSE>>=
    
    bernouilli_random_sample <- rbinom(n = 10000, size = 1, prob = 0.3)
    hist(bernouilli_random_sample)
    mean(bernouilli_random_sample); 0.3
    var(bernouilli_random_sample); 0.3*(1-0.3)
  @
  \pause
  Logistic regression in R:
  <<eval=FALSE>>=
    glm(formula = obs ~ 1 + x, family = "binomial", data=data)
  @
  
\end{frame}
%%%%%%%%%%%

\begin{frame}[fragile]{Logistic regression}
  
  %data prep
  <<echo=FALSE, eval=FALSE>>=
    set.seed(123)
    x <- rnorm(n = 200)  
    y <- 0.5 + 3*x
    obs <- sapply(y, FUN=function(x){rbinom(1,1, 1/(1+exp(-x)) ) })
    
    plot(x, obs)
    abline(lm(obs~x))
    
    summary(lm(obs~x))
    summary(glm(obs~x, family = "binomial"))
    
    dat <- data.frame(survival = obs, relative_size = x)
    write.csv(dat, file = "survivalsize.csv", quote = FALSE, row.names = FALSE)
  @



  \begin{exampleblock}{Exercise}
    \begin{enumerate}
      \item Load \texttt{survivalsize.csv}
      \item Plot survival data. What kind of distribution is it?
      \item Fit a linear model and a logistic model with intercept only. How to interpret the estimate?
      \item Fit a linear regression and a logistic regression of survival on relative size, compare the output
      \item Check the diagnostic plots for both models. Should you be worried?
      \item Extract and visualize a model prediction from both models (use the function predict, and/or do it by hand to practice link-function back-transformation)
    \end{enumerate}
  \end{exampleblock}

  <<echo=FALSE, eval=FALSE>>=
    dat <- read.csv("survivalsize.csv") 
    plot(dat$survival)
    hist(dat$survival)
    
    summary(glm(survival ~ 1, data=dat, family = "gaussian"))
    summary(glm(survival ~ 1, data=dat, family = "binomial"))
    1/(1+exp(-0.2209))
    mean(dat$survival)
    
    summary(lm1 <- glm(survival ~ 1 + relative_size, data=dat, family = "gaussian"))
    summary(glm1 <- glm(survival ~ 1 + relative_size, data=dat, family = "binomial"))
    1/(1+exp(-0.56-2.8078*mean(dat$relative_size)))

    plot(dat$relative_size, dat$survival, ylab="survival probability",
         xlab="relative size")
    ndat <- data.frame(relative_size = seq(-3,3, length.out = 100))  
    ndat <- cbind(ndat, predict(lm1, newdata = ndat),
                  predict(glm1, newdata = ndat, type = "response"))
    lines(ndat[,1], ndat[,2], col="red")
    lines(ndat[,1], ndat[,3], col="blue")
  @
\end{frame}
%%%%%%%%%%%

\begin{frame}{Logistic regression: Jensen's inequality}
\end{frame}
%%%%%%%%%%%

\begin{frame}{Logistic regression: trivia}

  \begin{itemize}[<+->]
    \item Many other link-functions, e.g. probit has nice properties to measure selection
    \item logit and inverse-logit functions in boot and mixtools packages
    \item exp(slope) is an odd-ratio 
    \item GLMs on binary data are more (or equally) powerful than proportion data
  \end{itemize}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[fragile]{Poisson regression}
  \begin{itemize}
    \item Count data
    \item Poisson distribution
    \item Link function: logarithm
    \item Inverse link function: exponential
    \item Linear function $y = intercept + slope_1 predictor_1 + slope_2 predictor_2 +$ \dots
  \end{itemize}
  
  \pause
  Poisson distribution in R:
  <<eval=FALSE>>=
    poisson_random_sample <- rpois(n = 10000, lambda = 4)  
    hist(poisson_random_sample)
    mean(poisson_random_sample)
    var(poisson_random_sample)
  @
  \pause
  Poisson regression in R:
  <<eval=FALSE>>=
    glm(formula = obs ~ 1 + x, family = "poisson", data = data)  
  @
\end{frame}
%%%%%%%%%%%

\begin{frame}[fragile]{Poisson regression}

<<eval=FALSE, echo=FALSE>>=
  x <-     
@
  
  
  \begin{exampleblock}{Exercise}
    \begin{enumerate}
      \item Load the data reproduction.csv
      \item Plot reproduction data, calculate the mean and variance. 
      \item Overlay a Gaussian distribution of same mean and variance, does it fit?
      \item Fit an compare a lm and a Poisson glm of reproduction on size 
      \item Check the diagnostic plots for both models. Should you be worried?
      \item Extract and visualize a model prediction from both models (use the function predict, and/or do it by hand to practice link-function back-transformation)
      \item Before GLMs, researchers used to log-transform the data and fit linear models. What are the problems with this approach?
    \end{enumerate}
  \end{exampleblock}

\end{frame}
%%%%%%%%%%%

\begin{frame}[fragile]{Poisson regression: over-dispersion}
  \begin{alertblock}{Sadly, simple Poisson models are almost always wrong in natura}
    \begin{itemize}[<+->]
      \item Poisson assumes: expected value = values variance
      \item True if all the sources of variation are in the model (almost impossible)
      \item Main risk:
        \begin{itemize}
          \item underestimate uncertainty 
          \item anti-conservative p-values
          \item find effects that are not real
        \end{itemize}
      \item Opposite risk rare but possible (e.g., bird clutch size)
      \item Solutions: relax mean/variance relationship
        \begin{itemize}
          \item Negative-binomial distribution 
          \item Quasi-Poisson (multiplicative over-dispersion)
          \item Observation-level random effect (additive over-dispersion)
        \end{itemize}
      \item \textbf{You should NEVER use glm(family $=$ poisson) again!}
    \end{itemize}
  \end{alertblock}
  
\end{frame}
%%%%%%%%%%%

 \begin{frame}[fragile]{Poisson regression: over-dispersion demonstrated}

  <<>>=
     set.seed(123) # random seed
     x <- rnorm(100) # predictor with no effect
     y <- exp(-1 + rnorm(100, 0, 2)) # variation of unknown origin
     obs <- sapply(y, FUN = function(y){
       rpois(n = 1, lambda = y)}) # generate Poisson data
   
     #plot(x, obs) # visualize data
     glm2 <- glm(obs ~ x, family = "poisson") #fit Poisson regression
     sglm2 <- summary(glm2) #look at the summary
     sglm2$coefficients[2,4] # the p-value 
     
     glm2q <- glm(obs ~ x, family = "quasipoisson") #quasiPoisson regression
     sglm2q <- summary(glm2q) #summary
     sglm2q$coefficients[2,4] # p-value
 @
 
\end{frame}
%%%%%%%%%%%

\begin{frame}[fragile]{Poisson regression: over-dispersion demonstrated}


 Maybe just bad luck?
\begin{exampleblock}{Exercise}
  Write a for loop to look at the distribution of p-values
\end{exampleblock}

<<eval=FALSE>>=
    set.seed( )
    poisson_pvalues <- vector(length = 1000)
    quasipoisson_pvalues <-
    for(i in  )
    {
      simulation + poisson and quasipoisson GLMs
    }
    hist( )
    mean( <0.05)
@

% 
%     <<echo=FALSE, eval=FALSE>>=
%     set.seed(123)
%     pvvector <- vector(length = 1000)
%     pvvectorq <- vector(length = 1000)
%     for (i in 1:1000)
%     {
%     x <- rnorm(100)
%     y <- exp(-1 + rnorm(100, 0, 2))
%     obs <- sapply(y, FUN = function(x){rpois(n = 1, lambda = x)})
%     glm2 <- glm(obs ~ x, family = "poisson")
%     sglm2 <- summary(glm2)
%     pvvector[i] <- sglm2$coefficients[2,4]
% 
%     glm2q <- glm(obs ~ x, family = "quasipoisson")
%     sglm2q <- summary(glm2q)
%     pvvectorq[i] <- sglm2q$coefficients[2,4]
%     }
%     hist(pvvector); mean(pvvector<0.05)
%     hist(pvvectorq) ; mean(pvvectorq<0.05)
%   @
% 

\end{frame}
%%%%%%%%%%%

\end{document}
